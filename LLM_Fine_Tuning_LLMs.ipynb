{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akimi-yano/data-science/blob/main/LLM_Fine_Tuning_LLMs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVa0caPZlogN"
      },
      "source": [
        "# Fine-Tuning of LLMs with Hugging Face"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fT5BjFcflZAh"
      },
      "source": [
        "## Step 1: Installing and importing the libraries for Hugging Face"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GLXwJqbjtPho"
      },
      "outputs": [],
      "source": [
        "!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRZm_OAbs3qA",
        "outputId": "0e3b347d-af83-493c-cd94-b2b7e326f00f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.24.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "!pip install huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nAMzy_0FtaUZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from trl import SFTTrainer\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, PeftModel\n",
        "from transformers import (AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, HfArgumentParser, TrainingArguments, pipeline, logging)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVu8hyHsw2nA",
        "outputId": "a622375f-b51b-4950-9a99-301b84b2fafb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "torch.cuda.is_available()  # Should return True if GPU is available"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMO9Ew1534fE"
      },
      "source": [
        "## Step 2: Setting up links to Hugging Face datasets and models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C4Rf3dfKllXJ"
      },
      "outputs": [],
      "source": [
        "# # Verison 1: Medical Knowledge\n",
        "\n",
        "# model_identifier = \"aboonaji/llama2finetune-v2\" # this is the model\n",
        "# source_dataset = \"gamino/wiki_medical_terms\" # this is the data set\n",
        "# formatted_dataset = \"aboonaji/wiki_medical_terms_llam2_format\" # this is the formatted data set as llama 2 expects a specific format"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Version 2: Languages\n",
        "\n",
        "model_identifier = \"aboonaji/llama2finetune-v2\" # this is the model\n",
        "formatted_dataset = \"mlabonne/guanaco-llama2\" # languages"
      ],
      "metadata": {
        "id": "ZoZ9wzR0YyqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQA3fMY647SE"
      },
      "source": [
        "## Step 3: Setting up all the QLoRA hyperparameters for fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KBuBVAPknE0g"
      },
      "outputs": [],
      "source": [
        "lora_hyper_r = 64\n",
        "lora_hyper_alpha = 16\n",
        "lora_hyper_dropout = 0.1 # 10%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFfD46YB2ffY"
      },
      "source": [
        "## Step 4: Setting up all the bitsandbytes hyperparameters for fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OXzgF5ufqMxz"
      },
      "outputs": [],
      "source": [
        "# quantization from 16 bits to 4 bits\n",
        "enable_4bit = True\n",
        "compute_dtype_bnb = \"float16\" # bnb = bits and bites\n",
        "quant_type_bnb = \"nf4\" # quantize to 4 bit precision\n",
        "double_quant_flag = False # do not apply quantization 2 times at 2 different stages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_66BRztm29wJ"
      },
      "source": [
        "## Step 5: Setting up all the training arguments hyperparameters for fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JaqbNdHqr7Ga"
      },
      "outputs": [],
      "source": [
        "results_dir = \"./results\"\n",
        "epochs_count = 10 # 10 is enough\n",
        "\n",
        "# by setting both of these to False, we make sure that we use the default precision of 32 bit\n",
        "enable_fp16 = False # dont use 16 bit floating point precision\n",
        "enable_bf16 = False # disable the brain floating point during training as well\n",
        "\n",
        "train_batch_size = 4\n",
        "eval_batch_size = 4\n",
        "gradient_accumulation_steps = 1 # number of gradient accumulation steps to increase the batch size without increasing the memory requirement\n",
        "checkpointing_flag = True # enable gradient checkpointing - technique to save memory with the cost of additional computation - useful for training large model\n",
        "grad_norm_limit = 0.3 # max norm of gradient\n",
        "train_learning_rate = 2e-4\n",
        "decay_rate = 0.001 # used for regularization to avoid overfitting - this is a small number\n",
        "optimizer_type = \"paged_adamw_32bit\" # optimizer to use - 32 bit precision version\n",
        "lr_scheduler_type = \"cosine\" # learning rate scheduler to stabilize training - use cosine curve\n",
        "steps_limit = 100\n",
        "warmup_percentage = 0.03 # 3% of the training steps will be used for warm up phase\n",
        "length_grouping = True # enable to group the training samples of similar length togehter <- this improves the training efficiency\n",
        "checkpoint_interval = 0 # we dont save any check point\n",
        "log_interval = 25 # how often log the intervals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cmmRIgc3Ree"
      },
      "source": [
        "## Step 6: Setting up all the supervised fine-tuning arguments hyperparameters for fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t9PzrratCiQb"
      },
      "outputs": [],
      "source": [
        "enable_packing = False # whether to use packing for our training or not - packing is a technique used in processing sequence\n",
        "# that multiple shorter sequences are combined into single training example to improve computational efficiency\n",
        "sequence_length_max = None # max sequence length for training\n",
        "device_assignment = {\"\":0} # device to use for training. Using CPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEsqc9gasp10"
      },
      "source": [
        "## Step 7: Loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DjrYQupeokiF",
        "outputId": "78f75246-d121-4f88-d559-8cdf2fa88fbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "training_data = load_dataset(formatted_dataset, split = \"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3oyTy-hHpdOq",
        "outputId": "3ee5d720-2af8-4e62-832d-8c949bd31b54"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['text'],\n",
              "    num_rows: 9846\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "training_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0tKj05usvyN"
      },
      "source": [
        "## Step 8: Defining the QLoRA configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjVkpW6-ppZg"
      },
      "outputs": [],
      "source": [
        "dtype_computation = getattr(torch, compute_dtype_bnb)\n",
        "\n",
        "bnb_setup = BitsAndBytesConfig(load_in_4bit = enable_4bit,\n",
        "                               bnb_4bit_quant_type = quant_type_bnb,\n",
        "                               bnb_4bit_use_double_quant = double_quant_flag,\n",
        "                               bnb_4bit_compute_dtype = dtype_computation\n",
        "                               )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rz3vMSzhs-P7"
      },
      "source": [
        "## Step 9: Loading the pre-trained LLaMA 2 model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140,
          "referenced_widgets": [
            "2f047f5f0f974429a66f4233b42a23cd",
            "8fa346c59fa440569dff6a4ca4aac49a",
            "a7fd818d6c8841e0a2a194b62b24c49d",
            "1de14351f2914c19b871d6103949a0f7",
            "bca7670f0c1944028935fe8cae939530",
            "9f1394c6cf514ea380ede5b76b115ecd",
            "4d7fb406e65c427fa8c006ee12ed4d34",
            "00bc8323c33640f4ab6e65d5b23158ec",
            "2d5f1978ead34509862499be22b65d28",
            "f1330b134b6146ca9fcfed4fceb87bf7",
            "633d7134bc0c47159086081c667d96b2"
          ]
        },
        "id": "J3-glUlkNKeC",
        "outputId": "9abb7c14-b67d-4616-ae3a-7643365fd5cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2f047f5f0f974429a66f4233b42a23cd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:460: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  return torch.load(checkpoint_file, map_location=\"cpu\")\n"
          ]
        }
      ],
      "source": [
        "llama_model = AutoModelForCausalLM.from_pretrained(model_identifier, quantization_config = bnb_setup, device_map = device_assignment)\n",
        "llama_model.config.use_case = False\n",
        "llama_model.config.pretraining_tp = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6aWb1e7tNRS"
      },
      "source": [
        "## Step 10: Loading the pre-trained tokenizer for the LLaMA 2 model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DbVfvGB3NLCv"
      },
      "outputs": [],
      "source": [
        "llama_tokenizer = AutoTokenizer.from_pretrained(model_identifier, trust_remote_code = True)\n",
        "llama_tokenizer.pad_token = llama_tokenizer.eos_token\n",
        "llama_tokenizer.padding_side = \"right\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxOOwMvStaDN"
      },
      "source": [
        "## Step 11: Setting up the configuration for the LoRA fine-tuning method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3hlwyUkINLyd"
      },
      "outputs": [],
      "source": [
        "peft_setup = LoraConfig(lora_alpha = lora_hyper_alpha,\n",
        "                        lora_dropout = lora_hyper_dropout,\n",
        "                        r = lora_hyper_r,\n",
        "                        bias = \"none\",\n",
        "                        task_type = \"CAUSAL_LM\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coUlIR-ytjiF"
      },
      "source": [
        "## Step 12: Creating a training configuration by setting the training parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8Y6IiP8NMil"
      },
      "outputs": [],
      "source": [
        "train_args = TrainingArguments(output_dir = results_dir,\n",
        "                               num_train_epochs = epochs_count,\n",
        "                               per_device_train_batch_size = train_batch_size,\n",
        "                               per_device_eval_batch_size = eval_batch_size,\n",
        "                               gradient_accumulation_steps = gradient_accumulation_steps,\n",
        "                               learning_rate = train_learning_rate,\n",
        "                               weight_decay = decay_rate,\n",
        "                               optim = optimizer_type,\n",
        "                               save_steps = checkpoint_interval,\n",
        "                               logging_steps = log_interval,\n",
        "                               fp16 = enable_fp16,\n",
        "                               bf16 = enable_bf16,\n",
        "                               max_grad_norm = grad_norm_limit,\n",
        "                               max_steps = steps_limit,\n",
        "                               warmup_ratio = warmup_percentage,\n",
        "                               group_by_length = length_grouping,\n",
        "                               lr_scheduler_type = lr_scheduler_type,\n",
        "                               gradient_checkpointing = checkpointing_flag\n",
        "                               )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-5-Thu0tpfu"
      },
      "source": [
        "## Step 13: Creating the Supervised Fine-Tuning Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j01UPOIvNNQL",
        "outputId": "52e502f8-a405-4a9d-ffde-43b3c72200af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/peft/utils/other.py:102: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:159: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "llama_sftt_trainer = SFTTrainer(model = llama_model,\n",
        "                                args = train_args,\n",
        "                                train_dataset = training_data,\n",
        "                                tokenizer = llama_tokenizer,\n",
        "                                peft_config = peft_setup,\n",
        "                                dataset_text_field = \"text\",\n",
        "                                max_seq_length = sequence_length_max,\n",
        "                                packing = enable_packing,\n",
        "                                )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # This is for debugging purposes\n",
        "\n",
        "# for name, param in llama_model.named_parameters():\n",
        "#     print(f\"{name}: requires_grad={param.requires_grad}\")"
      ],
      "metadata": {
        "id": "8-F2Y907L6ta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSF8SHFKt1xL"
      },
      "source": [
        "## Step 14: Training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "id": "aX0dVpAgNN77",
        "outputId": "8a69c509-3ce0-44d2-cfec-a40c08d9beeb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [100/100 01:31, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>1.447800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.580700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>1.150400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.469200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=100, training_loss=1.4120229530334472, metrics={'train_runtime': 95.0242, 'train_samples_per_second': 4.209, 'train_steps_per_second': 1.052, 'total_flos': 3352777517137920.0, 'train_loss': 1.4120229530334472, 'epoch': 0.04})"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "llama_sftt_trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMPw6WU6vbjP"
      },
      "source": [
        "## Step 15: Chatting with the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oi53BWPMNOrC"
      },
      "outputs": [],
      "source": [
        "# # Version 1: Medical Knowledge\n",
        "\n",
        "# user_prompt = \"Please tell me about Bursitis\"\n",
        "# text_generation_pipeline = pipeline(task = \"text-generation\", model = llama_model, tokenizer = llama_tokenizer, max_length = 300)\n",
        "# generation_result = text_generation_pipeline(f\"<s>[INST] {user_prompt} [/INST]\")\n",
        "# print(generation_result[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Version 2: Languages\n",
        "\n",
        "# user_prompt = \"How can we solve malapportionment in Malaysia?\"\n",
        "user_prompt = \"Is the FPTP electoral system a reason for malapportionment in Malaysia?\"\n",
        "text_generation_pipeline = pipeline(task = \"text-generation\", model = llama_model, tokenizer = llama_tokenizer, max_length = 300)\n",
        "generation_result = text_generation_pipeline(f\"<s>[INST] {user_prompt} [/INST]\")\n",
        "print(generation_result[0]['generated_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVz7DJFAZ_Du",
        "outputId": "22082668-039d-4762-89eb-bcbc6d9c2efa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
            "pip install xformers.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1270: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s>[INST] Is the FPTP electoral system a reason for malapportionment in Malaysia? [/INST] The FPTP electoral system is one of the reasons for malapportionment in Malaysia.\n",
            "\n",
            "Malapportionment is a phenomenon where the electoral system distributes seats in a legislative body unevenly, resulting in some constituencies having more representatives than others. In Malaysia, the FPTP system has led to malapportionment due to the unequal distribution of voters across constituencies.\n",
            "\n",
            "The FPTP system is based on a first-past-the-post system, where the candidate with the most votes wins the seat. However, this system can lead to distortions in the distribution of seats, particularly in cases where there are large disparities in the number of voters across constituencies. In Malaysia, the FPTP system has resulted in a situation where some constituencies have a much larger number of voters than others, leading to malapportionment.\n",
            "\n",
            "Malapportionment can have significant consequences for democracy, as it can lead to a lack of representation for certain groups of voters. In Malaysia, the malapportionment caused by the FPTP system has been criticized for leading to a lack of representation for the indigenous people of Sabah and Sarawak, as well as\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gxCD1ZRnyKOO"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLndi-XsxVRN"
      },
      "source": [
        "Notes: Efficient GPU Usage:\n",
        "\n",
        "Manage GPU usage to avoid consuming unnecessary resources and limit session time.\n",
        "\n",
        "1 Free GPU Memory: After using the GPU, free up memory to avoid errors:\n",
        "\n",
        "`torch.cuda.empty_cache()`\n",
        "\n",
        "2 Stop the runtime when done: If you're done using Colab for now, stop the runtime to prevent further usage of resources. Go to Runtime > Manage Sessions > Terminate any active sessions."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2f047f5f0f974429a66f4233b42a23cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8fa346c59fa440569dff6a4ca4aac49a",
              "IPY_MODEL_a7fd818d6c8841e0a2a194b62b24c49d",
              "IPY_MODEL_1de14351f2914c19b871d6103949a0f7"
            ],
            "layout": "IPY_MODEL_bca7670f0c1944028935fe8cae939530"
          }
        },
        "8fa346c59fa440569dff6a4ca4aac49a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9f1394c6cf514ea380ede5b76b115ecd",
            "placeholder": "​",
            "style": "IPY_MODEL_4d7fb406e65c427fa8c006ee12ed4d34",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "a7fd818d6c8841e0a2a194b62b24c49d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_00bc8323c33640f4ab6e65d5b23158ec",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2d5f1978ead34509862499be22b65d28",
            "value": 2
          }
        },
        "1de14351f2914c19b871d6103949a0f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f1330b134b6146ca9fcfed4fceb87bf7",
            "placeholder": "​",
            "style": "IPY_MODEL_633d7134bc0c47159086081c667d96b2",
            "value": " 2/2 [00:13&lt;00:00,  6.19s/it]"
          }
        },
        "bca7670f0c1944028935fe8cae939530": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9f1394c6cf514ea380ede5b76b115ecd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d7fb406e65c427fa8c006ee12ed4d34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "00bc8323c33640f4ab6e65d5b23158ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d5f1978ead34509862499be22b65d28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f1330b134b6146ca9fcfed4fceb87bf7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "633d7134bc0c47159086081c667d96b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}